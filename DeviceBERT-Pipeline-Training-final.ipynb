{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1AMHk6YsrI8jb31sSctRhQQR3HhJKiXPF","timestamp":1717628028544}],"gpuType":"A100","machine_shape":"hm","mount_file_id":"1AMHk6YsrI8jb31sSctRhQQR3HhJKiXPF","authorship_tag":"ABX9TyNRwpfzk6xGiQEAUYPTvQm2"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# INTRODUCTION - DeviceBERT: Using Enriched Named Entity Recognition to Identify Medical Device Terminology in Device Recalls Data.\n","\n","This notebook provides the code needed to prepare the data, train and implement DeviceBERT, a language model based on BioBERT, which is trained and finetuned to perform entity recognition of Medical Device terms.\n","\n","To prepare and train DeviceBERT, we create an enriched, BERT tokenizer, which is augmented with medical device vocabulary, curated from various Open FDA datasets. The BioBERT model is trained on a annotated corpus of Medical Device Recalls data, which is annotated in BIO format utiling Doccano.\n","\n","The Datasets and Model created from this project have been made available on Huggingface hub for use in further training and inferencing tasks:\n","\n","* **DeviceBERT Model Card:** https://huggingface.co/mfarrington/device_recalls_ner_model \\\n","* **NER Recalls Training Dataset:** https://huggingface.co/mfarrington/biobert-ner-fda-recalls-dataset\n","* **Enriched DeviceBERT Tokenizer:** https://huggingface.co/mfarrington/DeviceBERT-tokenizer\n","\n","**Background**: This project was created as a student final project for Stanford CS224N: Natural Language Proecessing with Deep Learning. In this project, we demonstrate that the challenges inherint in sub-domain specific NER tasks can be alleviated through an enriched approach to data preparation, tokenization and regularization techniques.\n","\n","**Paper:** *DeviceBERT: Addressing Domain-Specific Learning Challenges Using Vocabulary Enrichment to Identify Medical Device Terminology in FDA Recall Action Summaries*"],"metadata":{"id":"WlW4weUvryVM"}},{"cell_type":"markdown","source":[],"metadata":{"id":"48cNVKY2Q9Hy"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"zWN_91d2usWg"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)"]},{"cell_type":"markdown","source":[],"metadata":{"id":"o1k-lwq6fwLc"}},{"cell_type":"code","source":["from google.colab import userdata\n","userdata.get('HF_TOKEN')"],"metadata":{"id":"vEJtUqD7mo28"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Install additional dependencies\n","!pip install datasets\n","!pip install seqeval\n","!pip install evaluate\n","!pip install accelerate -U\n","!pip install huggingface_hub\n","!pip install transformers\n","!pip install nltk"],"metadata":{"id":"OMW1gYMFQ-gM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import warnings\n","warnings.filterwarnings(\"ignore\", category=UserWarning)\n","warnings.filterwarnings(\"ignore\", category=FutureWarning)"],"metadata":{"id":"eJ5j9xi491wU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import random\n","import nltk\n","from nltk.tokenize import word_tokenize\n","nltk.download('punkt')\n","import json\n","import re\n","import torch\n","from tqdm import tqdm\n","import tabulate as tb\n","from datasets import load_dataset, Dataset, load_metric\n","import evaluate\n","from transformers import (AutoModelForTokenClassification,\n","                          AutoTokenizer,\n","                          TrainingArguments,\n","                          DataCollatorForTokenClassification,\n","                          Trainer,\n","                          AdamW,\n","                          BertTokenizer,\n","                          BertForTokenClassification,\n","                          BertModel)"],"metadata":{"id":"i8z_rIHQwkSU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#copy the device recalls and annotation file to the working directory\n","!cp drive/MyDrive/Stanford/annotated-2000.jsonl .\n","!cp drive/MyDrive/Stanford/device-recall-0001-of-0001.json .\n","!cp drive/MyDrive/Stanford/foiclass.txt ."],"metadata":{"id":"jmwy4ZtYwzl3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#PART 1 - Preparing, Transforming and Tokenizing the Device Recalls Dataset\n","\n","This part of the notebook performs the data preparation, cleaning, tokenization and alignment for the NER dataset used to train the BioBERT model, which is created in Doccano. The dataset is created utilizing 2000 Device Recall actions annotated with NER tags for device recognition (B-DEVICE, I-DEVICE, O-DEVICE).\n","\n","Note: Data Annotation is a separate step that is performed in advance. To run this section of the notebook, you will need the original jsonl annotation file ('annotated-2000.jsonl'). Doccano is an open-source device annotation tool used to perform the NER annotations.To view the original annotations, or perform addtional annotations on the data, you can install and run doccano using the instructions here: https://doccano.github.io/doccano/."],"metadata":{"id":"li9Rl2EQtIpJ"}},{"cell_type":"code","source":["# Load the JSON Recalls data\n","with open('device-recall-0001-of-0001.json') as f:\n","  data = json.load(f)\n","\n","# Load the foiclass text data into a dataframe\n","foi_df = pd.read_csv('foiclass.txt', sep='|', header=0, encoding='latin1')\n","foi_df = foi_df[['DEVICENAME']].rename(columns={'DEVICENAME': 'device-name-foiclass'})"],"metadata":{"id":"y9NM7MYxqIHk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Extract the device name fields\n","cfres_id = []\n","device_name = []\n","product_description = []\n","recall_action = []\n","\n","rows = []\n","for result in data['results']:\n","    device_name = result.get('openfda', {}).get('device_name', None)\n","    recall_action = result.get('action', None)\n","    product_description = result.get('product_description', None)\n","    if device_name is not None:\n","        device_name = re.sub(r'[^a-zA-Z0-9\\s]', '', device_name)  # Remove special characters\n","    if recall_action is not None:\n","        recall_action = re.sub(r'[^a-zA-Z0-9\\s]', '', recall_action)  # Remove special characters\n","    if product_description is not None:\n","        product_description = re.sub(r'[^a-zA-Z0-9\\s]', '', product_description)  # Remove special characters\n","\n","    row = {\n","        'id': result.get('cfres_id'),\n","        'device_name': device_name,\n","        'product_description': product_description,\n","        'recall_action': recall_action\n","    }\n","    rows.append(row)\n","\n","# Create a dataframe and clean up special characters and extra spaces\n","df = pd.DataFrame(rows)\n","df = df.apply(lambda x: x.str.replace('\\n', ''))\n","df = df.applymap(lambda x: ' '.join(x.split()) if isinstance(x, str) else x)\n","print (df.size)\n","\n","#Break out each object into a dataframe and remove NAs\n","device_df = df[['id', 'device_name']].dropna(subset=['device_name'])\n","product_df = df[['id', 'product_description']].dropna(subset=['product_description'])\n","recall_df = df[['id', 'recall_action']].dropna(subset=['recall_action'])\n","foi_df = foi_df.dropna(subset=['device-name-foiclass'])"],"metadata":{"id":"MyqKILVEsG7m"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Tokenize each column into words\n","device_df['tokens'] = device_df['device_name'].apply(word_tokenize)\n","product_df['tokens'] = product_df['product_description'].apply(word_tokenize)\n","recall_df['tokens'] = recall_df['recall_action'].apply(word_tokenize)\n","foi_df['tokens'] = foi_df['device-name-foiclass'].apply(word_tokenize)\n","\n","# Print the resulting DataFrame\n","print (device_df.size)\n","print (product_df.size)\n","print (recall_df.size)\n","print (foi_df.size)"],"metadata":{"id":"25tlE79FcWP4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Merge the product and recall tokens, we'll remove values which are purely numeric from the vocab\n","token_df = pd.concat([device_df, product_df, recall_df, foi_df], ignore_index=True)\n","token_df = token_df[['tokens']]\n","token_df['tokens'] = token_df['tokens'].astype(str)\n","token_df = token_df[~token_df['tokens'].str.match('^[0-9]+$')]\n","\n","token_df = token_df.drop_duplicates(subset=['tokens'])\n","\n","\n","#shuffle the tokens, convert to list and remove duplicate tokens\n","token_vocab = token_df['tokens'].tolist()\n","\n","flattened_token_vocab = [item.strip(\"[]'\") for sublist in token_vocab for item in sublist.split(\"', '\")]\n","\n","print('BEFORE processing', len(flattened_token_vocab))\n","flattened_token_vocab = list(set(flattened_token_vocab)) #remove duplicate tokens\n","random.shuffle(flattened_token_vocab)\n","vocab_list = [element for element in flattened_token_vocab if not element.isdigit()]\n","print('AFTER Processing', len(vocab_list))"],"metadata":{"id":"QmglwdTNXEG_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Extract and Pre-Process the Annotated Device Recalls Data\n","In this step we load in the training data which has been previously annotated using Doccano with the BIO (Beginning, Inside, Outside) tags for medical devices (B-DEVICE, I-DEVICE, O-DEVICE) We perform validations to ensure the data contains no duplicate rows, and is formatted in the correct schema. We'll also drop the unneeded 'comments' column, which is used by Doccano."],"metadata":{"id":"IFsv4IcUzu0G"}},{"cell_type":"code","source":["filename = 'annotated-2000.jsonl'\n","\n","# Define the dataframe schema\n","schema = {\n","    'id': str,\n","    'text': str,\n","    'Comments': str,\n","    'label': object  # will hold a list of lists with the label info\n","}\n","\n","# Load the JSONL data into a DataFrame\n","df = pd.read_json(filename, lines=True, orient='records', dtype=schema)\n","\n","# Convert the 'label' JSONL column to a list of lists\n","df['label'] = df['label'].apply(lambda x: [tuple(l) for l in x])"],"metadata":{"id":"PIS7dtGSlGzF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df = df.drop('Comments', axis=1)\n","df = df.drop_duplicates(subset=['text'])\n","print(tb.tabulate(df.head(5), headers='keys', tablefmt='psql'))\n","print(df[\"id\"].value_counts())"],"metadata":{"id":"vJKEPT2gZTln"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Transform the tokenized data to a Dataset and apply NER Labels\n","In this step the dataframe is transformed into a dataset dictionary format which is tokenized and re-indexed to identify the correct label spans to apply the the NER tags from the annotations."],"metadata":{"id":"5hFuydY-amQD"}},{"cell_type":"code","source":["def transform_dataframe_to_dataset(df):\n","    dataset_dict = {\n","        \"id\": [],\n","        \"ner_tags\": [],\n","        \"tokens\": []\n","    }\n","\n","    for index, row in df.iterrows():\n","        # Add id\n","        dataset_dict[\"id\"].append(row[\"id\"])\n","\n","        # Tokenize text\n","        tokens = row[\"text\"].split()\n","\n","        # Initialize NER tags\n","        ner_tags = [\"O\"] * len(tokens)\n","\n","        # Process labels\n","        for label in row[\"label\"]:\n","            start, end, tag = label\n","            # Find token indices corresponding to label spans\n","            start_token_idx = None\n","            end_token_idx = None\n","            for i, token in enumerate(tokens):\n","                if start >= len(\" \".join(tokens[:i])):\n","                    start_token_idx = i\n","                if end <= len(\" \".join(tokens[:i + 1])):\n","                    end_token_idx = i + 1\n","                    break\n","            # Update NER tags with NER tag labels\n","            if start_token_idx is not None and end_token_idx is not None:\n","                ner_tags[start_token_idx] = f\"{tag}\"\n","                for i in range(start_token_idx + 1, end_token_idx):\n","                    ner_tags[i] = f\"{tag}\"\n","\n","        dataset_dict[\"tokens\"].append(tokens)\n","        dataset_dict[\"ner_tags\"].append(ner_tags)\n","\n","    return Dataset.from_dict(dataset_dict)"],"metadata":{"id":"6zPXVB4x2l-8"},"execution_count":null,"outputs":[]},{"source":["dataset = transform_dataframe_to_dataset(df) #create the dataset"],"cell_type":"code","metadata":{"id":"QvYCx3dJFBPk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Perform the label to ID mapping\n","In this step the NER labels are mapped to a correspondin numerical id, and mapped to the dataset using the existing label locations in the ner_tags element."],"metadata":{"id":"M7PwC_aErlu7"}},{"cell_type":"code","source":["# create a map of the expected ids to their labels\n","id2label = {\n","    0: 'O-DEVICE',\n","    1: 'B-DEVICE',\n","    2: 'I-DEVICE',\n","    -100: 'O',}\n","\n","# create a map of the expected labels to their ids\n","label2id = {\n","    'O-DEVICE': 0,\n","    'B-DEVICE': 1,\n","    'I-DEVICE': 2,\n","    'O': -100,}"],"metadata":{"id":"Hk3Aboj0f0TW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def transform_labels(dataset):\n","    new_labels = []\n","    labels = []\n","    for i in range(len(dataset[\"ner_tags\"])):\n","       new_label = label2id.get(dataset[\"ner_tags\"][i], -100)\n","       old_label = id2label.get(new_label, 'O')\n","       labels.append(old_label)\n","       new_labels.append(new_label)\n","    return {\"ner_tags\": new_labels, \"labels\": labels}\n","\n","# map string labels to IDs\n","dataset = dataset.map(transform_labels)"],"metadata":{"id":"Qxa6_Jt7w1-8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Create a Train/Test Split of the Dataset"],"metadata":{"id":"ul-dscAMix5e"}},{"cell_type":"code","source":["dataset = dataset.train_test_split(test_size=0.2, shuffle=True, ) #split the dataset into train and test sets"],"metadata":{"id":"XcaTEb9kipDp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Optional - Push Dataset to Hub\n","In this step we use the huggingface cli to push the updated dataset to the HuggingFace hub, making it easier to use in downstream tasks."],"metadata":{"id":"SOALLxYx7ve_"}},{"cell_type":"code","source":["!huggingface-cli login #enter token when prompted"],"metadata":{"id":"PzUKmjZTPbDX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#uncomment to push the datset to the hub\n","dataset.push_to_hub(\"mfarrington/biobert-ner-fda-recalls-dataset\")"],"metadata":{"id":"aTwZeRa27nQX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Load the FDA Recalls NER Dataset from Huggingface Hub\n","In this step we load the latest version of the recalls NER dataset created in the earlier process from the hub."],"metadata":{"id":"E-HEGm_XOUFa"}},{"cell_type":"code","source":["#load the dataset\n","hub_dataset = load_dataset(\"mfarrington/biobert-ner-fda-recalls-dataset\")"],"metadata":{"id":"G3MYQ9mOoHJl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Tokenize data using BERT Tokenizer and align the NER Labels\n","In this step, we'll use a pretrained DistilBERT tokenizer to tokenize the recalls data and evaluate how the tokenizer tokenizes medical device terminology\n","\n"],"metadata":{"id":"4hW2uxvrx7or"}},{"cell_type":"code","source":["# Load DistilBERT baseline tokenizer\n","tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-cased\", use_fast=True)"],"metadata":{"id":"v9EjiWMtCLgL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### In this step we test the tokenizer before expanding its vocabulary to observe how it resorts to sub-word tokenization of medical device terms\n","As can be seen from the example, the tokenizer performs sub-optimally on words not in its vocabulary; the medical device terms are tokenized into a series of subwords.\n"],"metadata":{"id":"pS_fJAINPE5A"}},{"cell_type":"code","source":["#Tokenize device terms\n","print(tokenizer.tokenize('Colonoscope'))\n","print(tokenizer.tokenize('Thoracolumbosacral'))\n","print(tokenizer.tokenize('Rhinoanemometer'))"],"metadata":{"id":"EFxbBXFU4D72"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Enhance BERT Tokenizer Vocabulary to include Medical Device terminology\n","In this section, we utilize the corpus of medical device terms extracted and prepared earlier to enrich the tokenizer's vocabulary. We perform experiments with different percentage splits of vocabulary data (100, 50, 25) to identify the best number of tokens to perform optimal tokenization."],"metadata":{"id":"fIgJ_kHx5UEl"}},{"cell_type":"code","source":["#Create vocabulary set splits of 100%, 50%, 25%\n","midpoint = len(vocab_list) // 2\n","vocab_list_50 = vocab_list[:midpoint]\n","random.shuffle(vocab_list_50)\n","midpoint = len(vocab_list_50) // 2\n","vocab_list_25 = vocab_list_50[:midpoint]\n","\n","print(len(vocab_list))\n","print(len(vocab_list_50))\n","print(len(vocab_list_25))\n","\n","\n","print(\"[BEFORE] ] tokenizer vocab size:\", len(tokenizer))\n","added_tokens = tokenizer.add_tokens(vocab_list)\n","print(\"[ AFTER ] tokenizer vocab size:\", len(tokenizer))\n","print('added_tokens:',added_tokens)"],"metadata":{"id":"W6YL4wFtc8NY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Verify the device terms are tokenized without subwords\n","print(tokenizer.tokenize('Colonoscope'))\n","print(tokenizer.tokenize('Thoracolumbosacral'))\n","print(tokenizer.tokenize('Rhinoanemometer'))"],"metadata":{"id":"mXAcCL4cjtlR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Optional - push the vocab enriched tokenizer to hub\n","tokenizer.save_pretrained('devicebert-tokenizer')\n","tokenizer.push_to_hub('devicebert-tokenizer')"],"metadata":{"id":"hUDr8fn4j4qa"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Realign the NER Labels and Tokens\n","Because the BERT tokenizer applies subword tokenization and special labels to the data, we need to realign the NER tags and BIO labels to ensure they accurately span the tokenized data. We also apply truncation to max lenght of the tokenizer model, in this case 512."],"metadata":{"id":"ybQuDFirPvLC"}},{"cell_type":"markdown","source":[],"metadata":{"id":"fLwO-gwLjtYj"}},{"cell_type":"code","source":["def tokenize_and_align_labels(examples):\n","    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n","\n","    labels = []\n","    ner_tags = []\n","    for i, label in enumerate(examples[f\"ner_tags\"]):\n","        word_ids = tokenized_inputs.word_ids(batch_index=i)  # Map tokens to their respective word.\n","        previous_word_idx = None\n","        label_ids = []\n","        for word_idx in word_ids:  # Set the special tokens to -100.\n","            if word_idx is None:\n","                label_ids.append(-100)\n","            elif word_idx != previous_word_idx:  # Only label the first token of a given word.\n","                label_ids.append(label[word_idx])\n","            else:\n","                label_ids.append(-100)\n","            previous_word_idx = word_idx\n","        labels.append(label_ids)\n","\n","    tokenized_inputs[\"ner_tags\"] = labels\n","    tokenized_inputs[\"labels\"] = labels\n","    return tokenized_inputs"],"metadata":{"id":"EkA2ijFsInX7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokenized_dataset = hub_dataset.map(tokenize_and_align_labels, batched=True)"],"metadata":{"id":"yKFm-o_XIzhT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def map_labels(example):\n","    labels = example['labels']\n","    mapped_labels = [id2label.get(label, 'O') for label in labels]\n","    example['labels'] = mapped_labels\n","    return example\n","\n","final_dataset = tokenized_dataset.map(map_labels)"],"metadata":{"id":"oiCxS8rP5KSJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(final_dataset['train'][1])"],"metadata":{"id":"cahLjpfn5Qu5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[],"metadata":{"id":"U7z73cKOagIB"}},{"cell_type":"markdown","source":["# PART 2 - Training, Finetuning, Regularizing and Evaluating the Model on the Annotated NER Dataset\n","\n","This section of the notebook performs the model training, and finetuning. To establish a baseline metric, we start with a BERT and BioBert Base Cased  transformers:\n","\n","* BERT:\n","* BioBERT: https://huggingface.co/dmis-lab/biobert-base-cased-v1.2\n","\n","Because of the relative small size of the Dataset (~2000 recall records), the baseline model overfits on the training. Therefore, we take additional steps to avoid overfitting through the application of several regularization techniques in this section. After training and evaluating the model performance to achieve acceptable results, the model is saved to the Hub and we can move on to Part 3 - Inferencing using the pretrained saved model.\n","\n","The Datasets and Model created from this project have been made available on Huggingface hub for use in further training and inferencing tasks.\n","\n","**Huggingface Model Card:** https://huggingface.co/mfarrington/device_recalls_ner_model. \\\n","**NER Recalls Training Dataset:** https://huggingface.co/mfarrington/biobert-ner-fda-recalls-dataset"],"metadata":{"id":"C1kZEHPcHUeP"}},{"cell_type":"markdown","source":["## Define a function to compute the model scores"],"metadata":{"id":"6_iZI2uCXK7A"}},{"cell_type":"code","source":["label_list = list(id2label.values()) #Extract the device labels\n","\n","def compute_metrics(p):\n","    predictions, labels = p\n","    predictions = np.argmax(predictions, axis=2)\n","\n","    true_predictions = [\n","        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n","        for prediction, label in zip(predictions, labels)\n","    ]\n","    true_labels = [\n","        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n","        for prediction, label in zip(predictions, labels)\n","    ]\n","\n","    results = seqeval.compute(predictions=true_predictions, references=true_labels)\n","\n","    return {\n","        \"precision\": results[\"overall_precision\"],\n","        \"recall\": results[\"overall_recall\"],\n","        \"f1\": results[\"overall_f1\"],\n","        \"accuracy\": results[\"overall_accuracy\"],\n","    }\n"],"metadata":{"id":"t07yxRnKi8Rh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Train and Finetune BioBERT on the Enhanced Vocabulary, with Regularization techniques."],"metadata":{"id":"KN0dlP3PcTEu"}},{"cell_type":"code","source":["#Initialize the evaluator\n","seqeval = evaluate.load(\"seqeval\")"],"metadata":{"id":"lo6C-v1Gc1cX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)"],"metadata":{"id":"JJsdvYV3csTC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Initialized biobert\n","deviceBERT_train_model = BertForTokenClassification.from_pretrained(\"dmis-lab/biobert-base-cased-v1.2\",\n","    num_labels=4,\n","    id2label=id2label,\n","    label2id=label2id,\n","    )\n","\n","#Resize the model based on the length of the BERT tokenizer after adding the new device vocabulary\n","deviceBERT_train_model.resize_token_embeddings(len(tokenizer))"],"metadata":{"id":"Yyqwib6PclkQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Apply dropout"],"metadata":{"id":"ETD1RTil0GkT"}},{"cell_type":"code","source":["import torch.nn as nn\n","dropout_prob = 0.1\n","\n","#Regularization - applying dropout to the BERT embeddings layer\n","deviceBERT_train_model.bert.embeddings.dropout = nn.Dropout(dropout_prob)\n","\n","#Regularization - applying dropout to the BERT encoding layer\n","deviceBERT_train_model.bert.encoder.dropout = nn.Dropout(dropout_prob)"],"metadata":{"id":"ByfNIIv80DrU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["training_args = TrainingArguments(\n","    output_dir=\"devicebert-base-cased-v1.0\",\n","    learning_rate=1e-5,\n","    per_device_train_batch_size=16,\n","    per_device_eval_batch_size=16,\n","    num_train_epochs=10,\n","    weight_decay=0.01,\n","    warmup_steps=500,\n","    eval_strategy=\"epoch\",\n","    save_strategy=\"epoch\",\n","    load_best_model_at_end=True,\n","    push_to_hub=True,\n","    bf16=True,\n",")"],"metadata":{"id":"lsexSo_8itAd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Define the optimizer\n","optimizer = AdamW(deviceBERT_train_model.parameters(), lr=training_args.learning_rate)"],"metadata":{"id":"IuukEL43Ab8L"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Perform Cross-Validation"],"metadata":{"id":"MzxQXd35kg0r"}},{"cell_type":"code","source":["from sklearn.model_selection import KFold\n","\n","# Use KFold for cross-validation\n","kf = KFold(n_splits=5, shuffle=True, random_state=42)\n","\n","# Store results for each fold\n","all_results = []\n","\n","\n","for train_index, val_index in kf.split(tokenized_dataset['train']):\n","    train_subsampler = Dataset.from_dict(tokenized_dataset['train'][train_index])\n","    val_subsampler = Dataset.from_dict(tokenized_dataset['train'][val_index])\n","\n","    trainer = Trainer(\n","        model=deviceBERT_train_model,\n","        args=training_args,\n","        train_dataset=train_subsampler,\n","        eval_dataset=val_subsampler,\n","        tokenizer=tokenizer,\n","        data_collator=data_collator,\n","        compute_metrics=compute_metrics,\n","        #optimizers=(optimizer, None)\n","    )\n","\n","    trainer.train()\n","\n","    results = trainer.evaluate()\n","    all_results.append(results)\n","\n","# Aggregate results\n","avg_precision = np.mean([result['eval_precision'] for result in all_results])\n","avg_recall = np.mean([result['eval_recall'] for result in all_results])\n","avg_f1 = np.mean([result['eval_f1'] for result in all_results])\n","avg_accuracy = np.mean([result['eval_accuracy'] for result in all_results])\n","\n","print(f\" Average Precision: {avg_precision}\")\n","print(f\" Average Recall: {avg_recall}\")\n","print(f\" Average F1: {avg_f1}\")\n","print(f\" Average Accuracy: {avg_accuracy}\")"],"metadata":{"id":"UpB5rcUm2k2t"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Capture the AdamW parameters\n","def print_adamw_params(trainer):\n","    print(\"AdamW Optimizer Parameters:\")\n","    print(f\"  Learning Rate (α): {trainer.args.learning_rate}\")\n","    print(f\"  Beta 1 (β1): {trainer.args.adam_beta1}\")\n","    print(f\"  Beta 2 (β2): {trainer.args.adam_beta2}\")\n","    print(f\"  Epsilon (ε): {trainer.args.adam_epsilon}\")\n","    print(f\"  Weight Decay (w): {trainer.args.weight_decay}\")\n","\n","print_adamw_params(trainer)"],"metadata":{"id":"RaBjOghkHIsG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Capture the batch size and input length\n","batch_size = trainer.args.per_device_train_batch_size\n","input_length = trainer.tokenizer.model_max_length\n","\n","print(f\"Batch Size: {batch_size}\")\n","print(f\"Input Length: {input_length}\")"],"metadata":{"id":"ct21vOlpOWcg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[],"metadata":{"id":"x6E8-RWbICTn"}},{"cell_type":"markdown","source":["#Define the Trainer class for remaining Experiments"],"metadata":{"id":"ZC7Y7hql6vYz"}},{"cell_type":"code","source":["trainer = Trainer(\n","    model=deviceBERT_train_model,\n","    args=training_args,\n","    train_dataset=tokenized_dataset[\"train\"],\n","    eval_dataset=tokenized_dataset[\"test\"],\n","    tokenizer=tokenizer,\n","    data_collator=data_collator,\n","    compute_metrics=compute_metrics,\n",")"],"metadata":{"id":"ZRaeqS4q6t5m"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["trainer.train()"],"metadata":{"id":"wCrBv655i7UP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["trainer.push_to_hub()"],"metadata":{"id":"0L_ex1io_7s5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Initialize the 3 models\n","In this step we define 3 models: our BERT baseline model, our bioBERT baseline model, and our deviceBERT model."],"metadata":{"id":"Zr6_1c1QgWET"}},{"cell_type":"code","source":["from transformers import AutoModelForTokenClassification\n","\n","bert_model = BertForTokenClassification.from_pretrained(\"google-bert/bert-base-cased\",\n","    num_labels=4,\n","    id2label=id2label,\n","    label2id=label2id)\n","\n","biobert_model = BertForTokenClassification.from_pretrained(\"dmis-lab/biobert-base-cased-v1.2\",\n","    num_labels=4,\n","    id2label=id2label,\n","    label2id=label2id)\n","\n","devicebert_model = BertForTokenClassification.from_pretrained(\"mfarrington/devicebert-base-cased-v1.0\",\n","    num_labels=4,\n","    id2label=id2label,\n","    label2id=label2id)\n","\n","model_dict = {}\n","model_dict.update({'bert model': bert_model, 'biobert model': biobert_model, 'devicebert model': devicebert_model})"],"metadata":{"id":"SWVgGnzsf7CE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Evaluate all 3 Model Performance (BERT base, BioBERT base, DeviceBERT) and record outputs."],"metadata":{"id":"0CYeDNwob4tM"}},{"cell_type":"code","source":["for key, value in model_dict.items():\n","  model_name = key\n","  model = value\n","\n","  training_args = TrainingArguments(\n","      output_dir=\"device_recalls_ner_baseline\",\n","      learning_rate=2e-5,\n","      per_device_train_batch_size=16,\n","      per_device_eval_batch_size=16,\n","      num_train_epochs=10,\n","      weight_decay=0.02,\n","      eval_strategy=\"epoch\",\n","      save_strategy=\"epoch\",\n","      load_best_model_at_end=True,\n","      push_to_hub=True,\n","      bf16=True,\n","  )\n","\n","  trainer = Trainer(\n","      model=model,\n","      args=training_args,\n","      train_dataset=tokenized_dataset[\"train\"],\n","      eval_dataset=tokenized_dataset[\"test\"],\n","      tokenizer=tokenizer,\n","      data_collator=data_collator,\n","      compute_metrics=compute_metrics,\n","  )\n","\n","  # Define the optimizer\n","  optimizer = AdamW(model.parameters(), lr=training_args.learning_rate)\n","\n","  print(f'Evaluating {model_name} .....')\n","  trainer.train()"],"metadata":{"id":"HUUupebWldUp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Create a pipeline and perform inferencing utilizing DeviceBERT on a new Recall Action"],"metadata":{"id":"wQZsHCZ3tz4_"}},{"cell_type":"code","source":["from transformers import pipeline\n","\n","# Load the NER pipeline\n","ner_pipeline = pipeline(\"ner\", model=\"mfarrington/devicebert-base-cased-v1.0\", tokenizer=tokenizer)\n","\n","# Input text\n","input_text = \"\"\"Philips Healthcare sent an URGENT-MEDICAL DEVICE RECALL letter dated October 15, 2012 to all affected customers.  The letter identified the product, problem, and actons to be taken by the c\n","ustomers. Customers were instructed to inspect all casters of the unit to ensure that they are all secured. If a caster is loose, customers were told to lock the caster in place, limit movement\n"," of the cart and contact their local Phillips Invivo Representative. Nevertheless, a Philips Invivo representative will contact the customer regarding their affected device.  All affected d\n","Philips Healthcare sent an \"URGENT-MEDICAL DEVICE RECALL\" letter dated October 15, 2012 to all affected customers.  The letter identified the product, problem, and actons to be taken by the\n","customers.  Customers were instructed to inspect all casters of the unit to ensure that they are all secured. If a caster is loose, customers were told to lock the caster in place, limit movement of the\n","cart and contact their local Phillips Invivo Representative. Nevertheless, a Philips Invivo representative will contact the customer regarding their affected device.  All affected devices will have new\n","casters installed in order to correct the problem.  Contact your local Philips Invivo Representative at 1-800-722-9377 for further information and support.\"\"\"\n","\n","# Perform NER prediction\n","ner_results = ner_pipeline(input_text)\n","labeled_tokens = []\n","\n","# Perform NER prediction with threshold of 99% probability\n","threshold = 0.99\n","ner_results = ner_pipeline(input_text)\n","\n","# Print labeled tokens\n","for result in ner_results:\n","    if result['score'] >= threshold:\n","        print(f\"Token: {result['word']} - Label: {result['entity']}\")"],"metadata":{"id":"0loGni9ety07"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Plot the combined score chart for all the experiments"],"metadata":{"id":"K44mTNLefAyT"}},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","\n","# Define the models and their corresponding precision, recall, and F1 scores\n","models = ['BERT', 'BioBERT', 'DeviceBERT (+Reg only)', 'DeviceBERT (+Vocab 100%)', 'DeviceBERT (+Vocab 50%)', 'DeviceBERT (+Vocab 25%)', 'DeviceBERT (+Reg+Vocab)']\n","precision = [72.96, 73.42, 85.14, 75.59, 81.56, 80.14, 82.37]\n","recall = [73.99, 73.29, 82.07, 73.46, 80.11, 77.87, 78.52]\n","f1_score = [73.47, 73.35, 83.56, 74.51, 80.83, 78.91, 80.37]\n","\n","# Sort the models based on F1 score\n","sorted_indices = sorted(range(len(f1_score)), key=lambda i: f1_score[i], reverse=True)\n","models_sorted = [models[i] for i in sorted_indices]\n","precision_sorted = [precision[i] for i in sorted_indices]\n","recall_sorted = [recall[i] for i in sorted_indices]\n","f1_score_sorted = [f1_score[i] for i in sorted_indices]\n","\n","# Highlighting the best performing model's text in red\n","best_model_index = f1_score_sorted.index(max(f1_score_sorted))\n","best_model = models_sorted[best_model_index]\n","\n","plt.figure(figsize=(8, 8))\n","\n","# Plotting precision, recall, and F1 score\n","plt.plot(range(len(models_sorted)), precision_sorted, marker='o', linestyle='-', label='Precision')\n","plt.plot(range(len(models_sorted)), recall_sorted, marker='o', linestyle='-', label='Recall')\n","plt.plot(range(len(models_sorted)), f1_score_sorted, marker='o', linestyle='-', label='F1 Score')\n","\n","# Annotate the best performing model with red color\n","plt.annotate(best_model, xy=(best_model_index, f1_score_sorted[best_model_index]),\n","             xytext=(best_model_index + 0.2, f1_score_sorted[best_model_index] - 1),\n","             arrowprops=dict(facecolor='red', arrowstyle='->'))\n","\n","# Set x-ticks as model names\n","plt.xticks(range(len(models_sorted)), models_sorted, rotation=45, ha='right')\n","\n","# Set labels and title\n","plt.ylabel('Scores (%)')\n","plt.xlabel('Model')\n","plt.title('Performance of NER Models')\n","\n","# Set legend\n","plt.legend()\n","\n","# Show plot\n","plt.grid(True)\n","plt.tight_layout()\n","plt.show()\n"],"metadata":{"id":"rUYDPuP7e8GA"},"execution_count":null,"outputs":[]}]}